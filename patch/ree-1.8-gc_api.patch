diff --git a/gc.c b/gc.c
index 9d2a264..942e2d1 100644
--- a/gc.c
+++ b/gc.c
@@ -18,6 +18,7 @@
 #include "node.h"
 #include "env.h"
 #include "re.h"
+#include "gc_api.h"
 #include <stdio.h>
 #include <setjmp.h>
 #include <math.h>
@@ -88,7 +89,19 @@ static size_t unstressed_malloc_limit = GC_MALLOC_LIMIT;
 
 static void run_final();
 static VALUE nomem_error;
+
+/*
+** Must invoke GC API callbacks when %SP is restored back to normal after
+** return from garbage_collect() because there is
+** not enough stack.  ruby_stack_check() will raise SystemStackError.
+** The MBARI version of forces %SP to a small reserved region.
+** This also means that callbacks inside garbage_collect() must not
+** cause the stack to overflow.
+*/
+static void garbage_collect_with_gc_api();
+static void garbage_collect_without_gc_api();
 static void garbage_collect();
+#define garbage_collect() garbage_collect_with_gc_api()
 
 
 /*
@@ -788,6 +801,12 @@ static RVALUE *himem, *lomem;
 #include "marktable.c"
 #include "fastmarktable.c"
 
+/* for gc_api.h */
+int rb_gc_markedQ(VALUE object)
+{
+  return rb_mark_table_contains((RVALUE *) object);
+}
+
 static int gc_cycles = 0;
 
 static void set_gc_parameters()
@@ -1752,6 +1771,8 @@ gc_sweep()
     int live_counts[256];
     int do_gc_stats = gc_statistics & verbose_gc_stats;
 
+    rb_gc_invoke_callbacks(RB_GC_PHASE_SWEEP, RB_GC_PHASE_BEFORE); 
+ 
     live_objects = 0;
 
     for (i = 0; i < heaps_used; i++) {
@@ -1864,10 +1885,14 @@ gc_sweep()
     }
     malloc_increase = 0;
     if (freed < free_min) {
+ 	rb_gc_invoke_callbacks(RB_GC_PHASE_ALLOC, RB_GC_PHASE_BEFORE);
 	add_heap();
+ 	rb_gc_invoke_callbacks(RB_GC_PHASE_ALLOC, RB_GC_PHASE_AFTER);
     }
     during_gc = 0;
     
+    rb_gc_invoke_callbacks(RB_GC_PHASE_SWEEP, RB_GC_PHASE_AFTER); 
+
     if (do_gc_stats) {
 	fprintf(gc_data_file, "objects processed: %.7d\n", live_objects+freed);
 	fprintf(gc_data_file, "live objects	: %.7d\n", live_objects);
@@ -1893,7 +1918,9 @@ gc_sweep()
 	    rb_thread_pending = 1;
 	}
 	if (!freelist) {
+            rb_gc_invoke_callbacks(RB_GC_PHASE_ALLOC, RB_GC_PHASE_BEFORE);
 	    add_heap();
+            rb_gc_invoke_callbacks(RB_GC_PHASE_ALLOC, RB_GC_PHASE_AFTER);
 	}
 	return;
     }
@@ -2119,6 +2146,8 @@ garbage_collect_0(VALUE *top_frame)
 	}
     }
 
+    rb_gc_invoke_callbacks(RB_GC_PHASE_MARK, RB_GC_PHASE_BEFORE);
+
     gc_stack_limit = __stack_grow(STACK_END, GC_LEVEL_MAX);
     rb_mark_table_prepare();
     init_mark_stack();
@@ -2211,6 +2240,9 @@ garbage_collect_0(VALUE *top_frame)
 	}
 	rb_gc_abort_threads();
     } while (!MARK_STACK_EMPTY);
+
+    rb_gc_invoke_callbacks(RB_GC_PHASE_MARK, RB_GC_PHASE_AFTER);
+
     gc_sweep();
     rb_mark_table_finalize();
     gc_cycles++;
@@ -2228,7 +2260,27 @@ garbage_collect_0(VALUE *top_frame)
 }
 
 static void
+garbage_collect_with_gc_api()
+{
+  if ( ! (dont_gc || during_gc) ) {
+    rb_gc_invoke_callbacks(RB_GC_PHASE_START, RB_GC_PHASE_BEFORE);
+    rb_gc_invoke_callbacks(RB_GC_PHASE_START, RB_GC_PHASE_AFTER);
+  }
+
+  garbage_collect_without_gc_api();
+
+  if ( ! (dont_gc || during_gc) ) {
+    rb_gc_invoke_callbacks(RB_GC_PHASE_END, RB_GC_PHASE_BEFORE);
+    rb_gc_invoke_callbacks(RB_GC_PHASE_END, RB_GC_PHASE_AFTER);
+  }
+}
+
+static void
+#ifdef garbage_collect
+garbage_collect_without_gc_api()
+#else
 garbage_collect()
+#endif
 {
   jmp_buf save_regs_gc_mark;
   VALUE *top;
@@ -2737,10 +2789,15 @@ rb_gc_finalize_deferred()
     RVALUE *p = deferred_final_list;
 
     deferred_final_list = 0;
+
+    rb_gc_invoke_callbacks(RB_GC_PHASE_FINALIZE, RB_GC_PHASE_BEFORE);
+
     if (p) {
 	finalize_list(p);
 	free_unused_heaps();
     }
+
+    rb_gc_invoke_callbacks(RB_GC_PHASE_FINALIZE, RB_GC_PHASE_AFTER);
 }
 
 void
@@ -2750,6 +2807,10 @@ rb_gc_call_finalizer_at_exit()
     struct heaps_slot *heap;
     int i;
 
+    rb_gc_invoke_callbacks(RB_GC_PHASE_AT_EXIT, RB_GC_PHASE_BEFORE);
+
+    rb_gc_invoke_callbacks(RB_GC_PHASE_FINALIZE, RB_GC_PHASE_BEFORE); 
+
     /* run finalizers */
     if (need_call_final && finalizer_table) {
 	p = deferred_final_list;
@@ -2795,6 +2856,10 @@ rb_gc_call_finalizer_at_exit()
 	    p++;
 	}
     }
+
+    rb_gc_invoke_callbacks(RB_GC_PHASE_FINALIZE, RB_GC_PHASE_AFTER);
+
+    rb_gc_invoke_callbacks(RB_GC_PHASE_AT_EXIT, RB_GC_PHASE_AFTER);
 }
 
 /*
